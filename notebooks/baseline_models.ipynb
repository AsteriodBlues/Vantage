{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models for F1 Race Prediction\n",
    "\n",
    "This notebook trains and evaluates baseline models:\n",
    "- Dummy baselines (mean, median, grid position)\n",
    "- Historical mean lookup\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Random Forest\n",
    "\n",
    "Goal: Establish baseline performance before trying advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from src.models import (\n",
    "    set_random_seeds,\n",
    "    load_modeling_data,\n",
    "    verify_data_integrity,\n",
    "    train_dummy_baseline,\n",
    "    train_mean_baseline,\n",
    "    train_linear_regression,\n",
    "    train_ridge_regression,\n",
    "    train_random_forest,\n",
    "    create_results_summary,\n",
    "    save_models\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Reproducibility\n",
    "set_random_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train/val/test splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_modeling_data()\n",
    "\n",
    "# Verify data quality\n",
    "verify_data_integrity(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick peek at features\n",
    "print(f\"\\nFeature columns ({len(X_train.columns)} total):\")\n",
    "print(X_train.columns.tolist()[:20])  # First 20\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dummy Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_results = train_dummy_baseline(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Historical Mean Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_results = train_mean_baseline(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_results = train_linear_regression(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = lr_results['feature_importance'].head(20)\n",
    "colors = ['red' if c < 0 else 'green' for c in top_features['coefficient']]\n",
    "ax.barh(range(len(top_features)), top_features['coefficient'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Coefficient Value')\n",
    "ax.set_title('Linear Regression: Top 20 Feature Coefficients')\n",
    "ax.axvline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic plots\n",
    "y_pred_train = lr_results['model'].predict(X_train)\n",
    "y_pred_val = lr_results['model'].predict(X_val)\n",
    "residuals_val = y_val - y_pred_val\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0, 0].scatter(y_pred_val, y_val, alpha=0.5)\n",
    "axes[0, 0].plot([0, 20], [0, 20], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Predicted Position')\n",
    "axes[0, 0].set_ylabel('Actual Position')\n",
    "axes[0, 0].set_title('Predicted vs Actual')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[0, 1].scatter(y_pred_val, residuals_val, alpha=0.5)\n",
    "axes[0, 1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Position')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residuals vs Predicted')\n",
    "\n",
    "# Residuals histogram\n",
    "axes[1, 0].hist(residuals_val, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Residuals Distribution')\n",
    "\n",
    "# Q-Q plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals_val, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_results = train_ridge_regression(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "cv_results = ridge_results['cv_results']\n",
    "alphas = cv_results['param_alpha'].data\n",
    "mean_scores = -cv_results['mean_test_score']  # Convert back to positive MAE\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alphas, mean_scores, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Alpha (Regularization Strength)')\n",
    "plt.ylabel('Cross-Validation MAE')\n",
    "plt.title('Ridge Regression: Hyperparameter Tuning')\n",
    "plt.grid(True, alpha=0.3)\n",
    "best_alpha = ridge_results['best_params']['alpha']\n",
    "plt.axvline(best_alpha, color='r', linestyle='--', label=f'Best: α={best_alpha}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Linear vs Ridge coefficients\n",
    "lr_coef = lr_results['feature_importance'].set_index('feature')['coefficient']\n",
    "ridge_coef = ridge_results['feature_importance'].set_index('feature')['coefficient']\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Linear': lr_coef,\n",
    "    'Ridge': ridge_coef\n",
    "}).head(20)\n",
    "\n",
    "comparison.plot(kind='barh', figsize=(10, 8), alpha=0.7)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Linear vs Ridge: Top 20 Coefficients')\n",
    "plt.axvline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = train_random_forest(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top features bar plot\n",
    "top_features = rf_results['feature_importance'].head(20)\n",
    "axes[0].barh(range(len(top_features)), top_features['importance'], alpha=0.7, color='forestgreen')\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels(top_features['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Random Forest: Top 20 Features')\n",
    "\n",
    "# Cumulative importance\n",
    "cumsum = rf_results['feature_importance']['importance'].cumsum()\n",
    "axes[1].plot(range(len(cumsum)), cumsum, linewidth=2)\n",
    "axes[1].axhline(0.90, color='r', linestyle='--', label='90% threshold')\n",
    "axes[1].axhline(0.95, color='orange', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('Cumulative Importance')\n",
    "axes[1].set_title('Cumulative Feature Importance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = {\n",
    "    'Linear Regression': lr_results,\n",
    "    'Ridge Regression': ridge_results,\n",
    "    'Random Forest': rf_results\n",
    "}\n",
    "\n",
    "summary = create_results_summary(all_results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# MAE comparison\n",
    "axes[0].barh(summary['Model'], summary['MAE'], alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('MAE (positions)')\n",
    "axes[0].set_title('Model Comparison: MAE')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# R² comparison\n",
    "axes[1].barh(summary['Model'], summary['R²'], alpha=0.7, color='forestgreen')\n",
    "axes[1].set_xlabel('R² Score')\n",
    "axes[1].set_title('Model Comparison: R²')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Training time vs MAE\n",
    "axes[2].scatter(summary['Train Time (s)'], summary['MAE'], s=150, alpha=0.6)\n",
    "for idx, row in summary.iterrows():\n",
    "    axes[2].annotate(row['Model'], (row['Train Time (s)'], row['MAE']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "axes[2].set_xlabel('Training Time (seconds)')\n",
    "axes[2].set_ylabel('MAE (positions)')\n",
    "axes[2].set_title('Efficiency: Training Time vs Performance')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model (Random Forest) for detailed error analysis\n",
    "best_model = rf_results['model']\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Create error dataframe\n",
    "error_df = pd.DataFrame({\n",
    "    'actual': y_val.values,\n",
    "    'predicted': y_pred,\n",
    "    'error': y_val.values - y_pred,\n",
    "    'abs_error': np.abs(y_val.values - y_pred),\n",
    "    'grid_position': X_val['GridPosition'].values\n",
    "})\n",
    "\n",
    "print(\"\\nError Statistics:\")\n",
    "print(error_df['abs_error'].describe())\n",
    "\n",
    "# Predictions within tolerance\n",
    "within_1 = (error_df['abs_error'] <= 1).mean() * 100\n",
    "within_3 = (error_df['abs_error'] <= 3).mean() * 100\n",
    "within_5 = (error_df['abs_error'] <= 5).mean() * 100\n",
    "\n",
    "print(f\"\\nPrediction Accuracy:\")\n",
    "print(f\"  Within 1 position: {within_1:.1f}%\")\n",
    "print(f\"  Within 3 positions: {within_3:.1f}%\")\n",
    "print(f\"  Within 5 positions: {within_5:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst predictions\n",
    "print(\"\\nWorst 10 Predictions:\")\n",
    "print(error_df.nlargest(10, 'abs_error')[['actual', 'predicted', 'error', 'grid_position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best predictions\n",
    "print(\"\\nBest 10 Predictions:\")\n",
    "print(error_df.nsmallest(10, 'abs_error')[['actual', 'predicted', 'error', 'grid_position']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error by grid position\n",
    "error_by_grid = error_df.groupby('grid_position')['abs_error'].mean().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(error_by_grid.index, error_by_grid.values, alpha=0.7, color='coral')\n",
    "plt.xlabel('Grid Position')\n",
    "plt.ylabel('Average Absolute Error')\n",
    "plt.title('Prediction Error by Grid Position')\n",
    "plt.xticks(range(1, 21))\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted scatter with color by grid position\n",
    "plt.figure(figsize=(10, 10))\n",
    "scatter = plt.scatter(y_pred, y_val, c=X_val['GridPosition'], \n",
    "                     cmap='RdYlGn_r', alpha=0.6, s=50)\n",
    "plt.plot([0, 20], [0, 20], 'k--', lw=2, label='Perfect prediction')\n",
    "plt.plot([0, 20], [1, 21], 'r--', lw=1, alpha=0.5, label='±1 position')\n",
    "plt.plot([0, 20], [-1, 19], 'r--', lw=1, alpha=0.5)\n",
    "plt.plot([0, 20], [3, 23], 'orange', linestyle='--', lw=1, alpha=0.5, label='±3 positions')\n",
    "plt.plot([0, 20], [-3, 17], 'orange', linestyle='--', lw=1, alpha=0.5)\n",
    "plt.colorbar(scatter, label='Grid Position')\n",
    "plt.xlabel('Predicted Finish Position')\n",
    "plt.ylabel('Actual Finish Position')\n",
    "plt.title('Random Forest: Predicted vs Actual (colored by grid position)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "save_models(all_results)\n",
    "\n",
    "# Save summary table\n",
    "summary.to_csv('../results/models/baseline_results.csv', index=False)\n",
    "print(\"\\nResults saved to results/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "Document your observations here:\n",
    "\n",
    "1. **Baseline Performance**: Grid position baseline achieves MAE of ~X positions\n",
    "2. **Linear Models**: Linear/Ridge regression achieves MAE of ~X positions\n",
    "3. **Tree Models**: Random Forest achieves MAE of ~X positions\n",
    "4. **Feature Importance**: GridPosition, circuit stats, and team performance dominate\n",
    "5. **Error Patterns**: Errors are higher for [front/back] of grid, especially at [circuit type]\n",
    "6. **Next Steps**: XGBoost and hyperparameter tuning should push below X.X MAE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
