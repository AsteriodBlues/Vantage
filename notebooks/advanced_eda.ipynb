{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced EDA & Statistical Testing\n",
    "\n",
    "Deep statistical analysis of grid position effects, team performance, and validation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, ttest_ind, pearsonr, spearmanr\n",
    "from scipy.stats import f_oneway, chi2_contingency, kstest, levene\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('../data/processed/processed_race_data.csv')\n",
    "\n",
    "df_all = pd.read_csv(data_path)\n",
    "df_finished = df_all[df_all['completed_race'] == True].copy()\n",
    "\n",
    "print(f\"Total records: {len(df_all):,}\")\n",
    "print(f\"Finished races: {len(df_finished):,}\")\n",
    "print(f\"Years: {sorted(df_all['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Grid Side Analysis (Clean vs Dirty)\n",
    "\n",
    "Odd positions (P1, P3, P5...) are on the racing line (clean). Even positions (P2, P4, P6...) are off-line (dirty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add grid side classification\n",
    "df_finished['grid_side'] = df_finished['GridPosition'].apply(\n",
    "    lambda x: 'clean' if x % 2 == 1 else 'dirty'\n",
    ")\n",
    "\n",
    "print(\"Grid side distribution:\")\n",
    "print(df_finished['grid_side'].value_counts())\n",
    "\n",
    "# Overall comparison\n",
    "clean_stats = df_finished[df_finished['grid_side'] == 'clean']['position_change'].describe()\n",
    "dirty_stats = df_finished[df_finished['grid_side'] == 'dirty']['position_change'].describe()\n",
    "\n",
    "print(\"\\nPosition change by grid side:\")\n",
    "print(\"\\nClean side (odd positions):\")\n",
    "print(clean_stats)\n",
    "print(\"\\nDirty side (even positions):\")\n",
    "print(dirty_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired comparison - compare each row of grid\n",
    "paired_analysis = []\n",
    "\n",
    "for row in range(1, 11):  # Rows 1-10 (P1/P2, P3/P4, ... P19/P20)\n",
    "    clean_pos = row * 2 - 1  # Odd position\n",
    "    dirty_pos = row * 2      # Even position\n",
    "    \n",
    "    clean_data = df_finished[df_finished['GridPosition'] == clean_pos]\n",
    "    dirty_data = df_finished[df_finished['GridPosition'] == dirty_pos]\n",
    "    \n",
    "    if len(clean_data) > 0 and len(dirty_data) > 0:\n",
    "        clean_avg_change = clean_data['position_change'].mean()\n",
    "        dirty_avg_change = dirty_data['position_change'].mean()\n",
    "        \n",
    "        clean_avg_finish = clean_data['Position'].mean()\n",
    "        dirty_avg_finish = dirty_data['Position'].mean()\n",
    "        \n",
    "        # Advantage is how much better clean side performs\n",
    "        advantage = dirty_avg_change - clean_avg_change\n",
    "        \n",
    "        paired_analysis.append({\n",
    "            'row': row,\n",
    "            'clean_pos': clean_pos,\n",
    "            'dirty_pos': dirty_pos,\n",
    "            'clean_avg_change': clean_avg_change,\n",
    "            'dirty_avg_change': dirty_avg_change,\n",
    "            'clean_advantage': advantage,\n",
    "            'clean_avg_finish': clean_avg_finish,\n",
    "            'dirty_avg_finish': dirty_avg_finish\n",
    "        })\n",
    "\n",
    "paired_df = pd.DataFrame(paired_analysis)\n",
    "\n",
    "print(\"\\nPaired grid row analysis:\")\n",
    "print(paired_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nOverall clean side advantage: {paired_df['clean_advantage'].mean():.3f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-test\n",
    "clean_changes = df_finished[df_finished['grid_side'] == 'clean']['position_change'].values\n",
    "dirty_changes = df_finished[df_finished['grid_side'] == 'dirty']['position_change'].values\n",
    "\n",
    "# Since not truly paired (different sample sizes), use independent t-test\n",
    "t_stat, p_value = ttest_ind(clean_changes, dirty_changes)\n",
    "\n",
    "print(f\"\\nStatistical test (clean vs dirty):\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Clean side advantage is statistically significant\")\n",
    "else:\n",
    "    print(\"Result: No statistically significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid side advantage by circuit\n",
    "circuit_grid_side = []\n",
    "\n",
    "for circuit in df_finished['circuit'].unique():\n",
    "    circuit_data = df_finished[df_finished['circuit'] == circuit]\n",
    "    \n",
    "    clean = circuit_data[circuit_data['grid_side'] == 'clean']['position_change'].mean()\n",
    "    dirty = circuit_data[circuit_data['grid_side'] == 'dirty']['position_change'].mean()\n",
    "    \n",
    "    advantage = dirty - clean\n",
    "    \n",
    "    circuit_grid_side.append({\n",
    "        'circuit': circuit,\n",
    "        'clean_avg': clean,\n",
    "        'dirty_avg': dirty,\n",
    "        'clean_advantage': advantage\n",
    "    })\n",
    "\n",
    "grid_side_df = pd.DataFrame(circuit_grid_side).sort_values('clean_advantage', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "colors = ['green' if x > 0 else 'red' for x in grid_side_df['clean_advantage']]\n",
    "ax.barh(grid_side_df['circuit'], grid_side_df['clean_advantage'],\n",
    "        color=colors, edgecolor='black', alpha=0.7)\n",
    "\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Clean Side Advantage (positions)', fontweight='bold')\n",
    "ax.set_ylabel('Circuit', fontweight='bold')\n",
    "ax.set_title('Grid Side Effect by Circuit', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 circuits with strongest clean side advantage:\")\n",
    "print(grid_side_df.head(5)[['circuit', 'clean_advantage']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Team Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance metrics\n",
    "team_stats = []\n",
    "\n",
    "for team in df_finished['TeamName'].unique():\n",
    "    team_data = df_finished[df_finished['TeamName'] == team]\n",
    "    team_all = df_all[df_all['TeamName'] == team]\n",
    "    \n",
    "    avg_grid = team_data['GridPosition'].mean()\n",
    "    avg_finish = team_data['Position'].mean()\n",
    "    performance_delta = avg_grid - avg_finish  # Positive = gaining positions\n",
    "    \n",
    "    consistency = team_data['Position'].std()\n",
    "    dnf_rate = (team_all['is_dnf'].sum() / len(team_all) * 100)\n",
    "    \n",
    "    num_races = len(team_all)\n",
    "    wins = (team_data['Position'] == 1).sum()\n",
    "    podiums = (team_data['Position'] <= 3).sum()\n",
    "    points_finishes = (team_data['Position'] <= 10).sum()\n",
    "    \n",
    "    win_rate = (wins / num_races * 100) if num_races > 0 else 0\n",
    "    podium_rate = (podiums / num_races * 100) if num_races > 0 else 0\n",
    "    points_rate = (points_finishes / num_races * 100) if num_races > 0 else 0\n",
    "    \n",
    "    team_stats.append({\n",
    "        'team': team,\n",
    "        'races': num_races,\n",
    "        'avg_grid': avg_grid,\n",
    "        'avg_finish': avg_finish,\n",
    "        'performance_delta': performance_delta,\n",
    "        'consistency': consistency,\n",
    "        'dnf_rate': dnf_rate,\n",
    "        'win_rate': win_rate,\n",
    "        'podium_rate': podium_rate,\n",
    "        'points_rate': points_rate\n",
    "    })\n",
    "\n",
    "team_df = pd.DataFrame(team_stats).sort_values('performance_delta', ascending=False)\n",
    "\n",
    "print(\"Team Performance Analysis:\")\n",
    "print(team_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance scatter plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Color code by performance tier\n",
    "def get_tier_color(avg_finish):\n",
    "    if avg_finish <= 5:\n",
    "        return 'gold'\n",
    "    elif avg_finish <= 10:\n",
    "        return 'silver'\n",
    "    else:\n",
    "        return '#cd7f32'\n",
    "\n",
    "colors = [get_tier_color(x) for x in team_df['avg_finish']]\n",
    "\n",
    "scatter = ax.scatter(team_df['avg_grid'], team_df['avg_finish'],\n",
    "                     s=team_df['races'] * 2, alpha=0.6,\n",
    "                     c=colors, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Reference line (no change)\n",
    "ax.plot([0, 20], [0, 20], 'r--', alpha=0.5, linewidth=2, label='No change')\n",
    "\n",
    "# Label teams\n",
    "for idx, row in team_df.iterrows():\n",
    "    ax.annotate(row['team'], (row['avg_grid'], row['avg_finish']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Average Grid Position', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Team Performance: Grid vs Finish', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(2, 18, 'Over-performing', fontsize=11, style='italic', color='green')\n",
    "ax.text(18, 2, 'Under-performing', fontsize=11, style='italic', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position change by team\n",
    "sorted_teams = team_df.sort_values('performance_delta', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "colors_perf = ['green' if x > 0 else 'red' for x in sorted_teams['performance_delta']]\n",
    "ax.barh(sorted_teams['team'], sorted_teams['performance_delta'],\n",
    "        color=colors_perf, edgecolor='black', alpha=0.7)\n",
    "\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Average Position Change', fontweight='bold')\n",
    "ax.set_ylabel('Team', fontweight='bold')\n",
    "ax.set_title('Team Performance Efficiency', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 over-performing teams:\")\n",
    "print(team_df.head(5)[['team', 'performance_delta', 'avg_grid', 'avg_finish']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance over time (major teams)\n",
    "major_teams = ['Mercedes', 'Red Bull Racing', 'Ferrari', 'McLaren', 'Alpine']\n",
    "available_major = [t for t in major_teams if t in df_finished['TeamName'].values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for team in available_major:\n",
    "    team_yearly = []\n",
    "    \n",
    "    for year in sorted(df_finished['year'].unique()):\n",
    "        year_team = df_finished[(df_finished['year'] == year) & \n",
    "                                (df_finished['TeamName'] == team)]\n",
    "        if len(year_team) > 0:\n",
    "            avg_finish = year_team['Position'].mean()\n",
    "            team_yearly.append({'year': year, 'avg_finish': avg_finish})\n",
    "    \n",
    "    if len(team_yearly) > 0:\n",
    "        team_yearly_df = pd.DataFrame(team_yearly)\n",
    "        ax.plot(team_yearly_df['year'], team_yearly_df['avg_finish'],\n",
    "                marker='o', linewidth=2, markersize=6, label=team)\n",
    "\n",
    "ax.set_xlabel('Year', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Team Performance Evolution (2018-2024)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.invert_yaxis()  # Lower is better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive statistical testing\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Correlation tests\n",
    "print(\"\\n1. CORRELATION TESTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "pearson_r, pearson_p = pearsonr(df_finished['GridPosition'], df_finished['Position'])\n",
    "print(f\"\\nPearson correlation:\")\n",
    "print(f\"  r = {pearson_r:.4f}\")\n",
    "print(f\"  p-value = {pearson_p:.6f}\")\n",
    "print(f\"  R² = {pearson_r**2:.4f} ({pearson_r**2*100:.1f}% variance explained)\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'Pearson Correlation',\n",
    "    'statistic': pearson_r,\n",
    "    'p_value': pearson_p,\n",
    "    'significant': pearson_p < 0.05\n",
    "})\n",
    "\n",
    "spearman_r, spearman_p = spearmanr(df_finished['GridPosition'], df_finished['Position'])\n",
    "print(f\"\\nSpearman rank correlation:\")\n",
    "print(f\"  ρ = {spearman_r:.4f}\")\n",
    "print(f\"  p-value = {spearman_p:.6f}\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'Spearman Correlation',\n",
    "    'statistic': spearman_r,\n",
    "    'p_value': spearman_p,\n",
    "    'significant': spearman_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ANOVA - finish positions different across grid positions?\n",
    "print(\"\\n2. ANOVA TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "grid_groups = [df_finished[df_finished['GridPosition'] == i]['Position'].values \n",
    "               for i in range(1, 21) if len(df_finished[df_finished['GridPosition'] == i]) > 0]\n",
    "\n",
    "f_stat, anova_p = f_oneway(*grid_groups)\n",
    "\n",
    "print(f\"\\nOne-way ANOVA (finish position by grid position):\")\n",
    "print(f\"  F-statistic = {f_stat:.4f}\")\n",
    "print(f\"  p-value = {anova_p:.6f}\")\n",
    "\n",
    "if anova_p < 0.05:\n",
    "    print(f\"  Result: Finish positions significantly differ across grid positions\")\n",
    "else:\n",
    "    print(f\"  Result: No significant difference\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'ANOVA',\n",
    "    'statistic': f_stat,\n",
    "    'p_value': anova_p,\n",
    "    'significant': anova_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Chi-square tests\n",
    "print(\"\\n3. CHI-SQUARE TESTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Pole position advantage\n",
    "pole_data = df_finished[df_finished['GridPosition'] == 1]\n",
    "pole_wins = (pole_data['Position'] == 1).sum()\n",
    "pole_non_wins = len(pole_data) - pole_wins\n",
    "\n",
    "# Expected if random (1/20 chance)\n",
    "expected_wins = len(pole_data) / 20\n",
    "expected_non_wins = len(pole_data) - expected_wins\n",
    "\n",
    "chi2_pole, p_pole = stats.chisquare([pole_wins, pole_non_wins], \n",
    "                                     [expected_wins, expected_non_wins])\n",
    "\n",
    "print(f\"\\nPole position advantage vs random chance:\")\n",
    "print(f\"  Observed wins: {pole_wins}\")\n",
    "print(f\"  Expected wins (random): {expected_wins:.1f}\")\n",
    "print(f\"  χ² = {chi2_pole:.4f}\")\n",
    "print(f\"  p-value = {p_pole:.6f}\")\n",
    "\n",
    "if p_pole < 0.05:\n",
    "    print(f\"  Result: Pole advantage is statistically significant\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'Chi-Square (Pole)',\n",
    "    'statistic': chi2_pole,\n",
    "    'p_value': p_pole,\n",
    "    'significant': p_pole < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Normality test on position change\n",
    "print(\"\\n4. NORMALITY TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "ks_stat, ks_p = kstest(df_finished['position_change'], 'norm',\n",
    "                       args=(df_finished['position_change'].mean(),\n",
    "                             df_finished['position_change'].std()))\n",
    "\n",
    "print(f\"\\nKolmogorov-Smirnov test (position change normality):\")\n",
    "print(f\"  KS-statistic = {ks_stat:.4f}\")\n",
    "print(f\"  p-value = {ks_p:.6f}\")\n",
    "\n",
    "if ks_p < 0.05:\n",
    "    print(f\"  Result: Position change is NOT normally distributed\")\n",
    "else:\n",
    "    print(f\"  Result: Position change follows normal distribution\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'K-S Normality',\n",
    "    'statistic': ks_stat,\n",
    "    'p_value': ks_p,\n",
    "    'significant': ks_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Variance equality test\n",
    "print(\"\\n5. VARIANCE EQUALITY TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Compare variance across first 5 grid positions\n",
    "variance_groups = [df_finished[df_finished['GridPosition'] == i]['position_change'].values \n",
    "                   for i in range(1, 6)]\n",
    "\n",
    "lev_stat, lev_p = levene(*variance_groups)\n",
    "\n",
    "print(f\"\\nLevene's test (variance equality across P1-P5):\")\n",
    "print(f\"  Statistic = {lev_stat:.4f}\")\n",
    "print(f\"  p-value = {lev_p:.6f}\")\n",
    "\n",
    "if lev_p < 0.05:\n",
    "    print(f\"  Result: Variances are NOT equal across grid positions\")\n",
    "else:\n",
    "    print(f\"  Result: Variances are equal\")\n",
    "\n",
    "results.append({\n",
    "    'test': \"Levene's Test\",\n",
    "    'statistic': lev_stat,\n",
    "    'p_value': lev_p,\n",
    "    'significant': lev_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL TEST SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nSignificant results: {results_df['significant'].sum()}/{len(results_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
