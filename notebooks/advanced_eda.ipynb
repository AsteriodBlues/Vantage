{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced EDA & Statistical Testing\n",
    "\n",
    "Deep statistical analysis of grid position effects, team performance, and validation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, ttest_ind, pearsonr, spearmanr\n",
    "from scipy.stats import f_oneway, chi2_contingency, kstest, levene\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('../data/processed/processed_race_data.csv')\n",
    "\n",
    "df_all = pd.read_csv(data_path)\n",
    "df_finished = df_all[df_all['completed_race'] == True].copy()\n",
    "\n",
    "print(f\"Total records: {len(df_all):,}\")\n",
    "print(f\"Finished races: {len(df_finished):,}\")\n",
    "print(f\"Years: {sorted(df_all['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Grid Side Analysis (Clean vs Dirty)\n",
    "\n",
    "Odd positions (P1, P3, P5...) are on the racing line (clean). Even positions (P2, P4, P6...) are off-line (dirty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add grid side classification\n",
    "df_finished['grid_side'] = df_finished['GridPosition'].apply(\n",
    "    lambda x: 'clean' if x % 2 == 1 else 'dirty'\n",
    ")\n",
    "\n",
    "print(\"Grid side distribution:\")\n",
    "print(df_finished['grid_side'].value_counts())\n",
    "\n",
    "# Overall comparison\n",
    "clean_stats = df_finished[df_finished['grid_side'] == 'clean']['position_change'].describe()\n",
    "dirty_stats = df_finished[df_finished['grid_side'] == 'dirty']['position_change'].describe()\n",
    "\n",
    "print(\"\\nPosition change by grid side:\")\n",
    "print(\"\\nClean side (odd positions):\")\n",
    "print(clean_stats)\n",
    "print(\"\\nDirty side (even positions):\")\n",
    "print(dirty_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired comparison - compare each row of grid\n",
    "paired_analysis = []\n",
    "\n",
    "for row in range(1, 11):  # Rows 1-10 (P1/P2, P3/P4, ... P19/P20)\n",
    "    clean_pos = row * 2 - 1\n",
    "    dirty_pos = row * 2\n",
    "    \n",
    "    clean_data = df_finished[df_finished['GridPosition'] == clean_pos]\n",
    "    dirty_data = df_finished[df_finished['GridPosition'] == dirty_pos]\n",
    "    \n",
    "    if len(clean_data) > 0 and len(dirty_data) > 0:\n",
    "        clean_avg_change = clean_data['position_change'].mean()\n",
    "        dirty_avg_change = dirty_data['position_change'].mean()\n",
    "        advantage = dirty_avg_change - clean_avg_change\n",
    "        \n",
    "        paired_analysis.append({\n",
    "            'row': row,\n",
    "            'clean_pos': clean_pos,\n",
    "            'dirty_pos': dirty_pos,\n",
    "            'clean_avg_change': clean_avg_change,\n",
    "            'dirty_avg_change': dirty_avg_change,\n",
    "            'clean_advantage': advantage\n",
    "        })\n",
    "\n",
    "paired_df = pd.DataFrame(paired_analysis)\n",
    "print(\"\\nPaired grid row analysis:\")\n",
    "print(paired_df.to_string(index=False))\n",
    "print(f\"\\nOverall clean side advantage: {paired_df['clean_advantage'].mean():.3f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-test\n",
    "clean_changes = df_finished[df_finished['grid_side'] == 'clean']['position_change'].values\n",
    "dirty_changes = df_finished[df_finished['grid_side'] == 'dirty']['position_change'].values\n",
    "\n",
    "t_stat, p_value = ttest_ind(clean_changes, dirty_changes)\n",
    "\n",
    "print(f\"\\nStatistical test (clean vs dirty):\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Clean side advantage is statistically significant\")\n",
    "else:\n",
    "    print(\"Result: No statistically significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid side advantage by circuit\n",
    "circuit_grid_side = []\n",
    "\n",
    "for circuit in df_finished['circuit'].unique():\n",
    "    circuit_data = df_finished[df_finished['circuit'] == circuit]\n",
    "    clean = circuit_data[circuit_data['grid_side'] == 'clean']['position_change'].mean()\n",
    "    dirty = circuit_data[circuit_data['grid_side'] == 'dirty']['position_change'].mean()\n",
    "    \n",
    "    circuit_grid_side.append({\n",
    "        'circuit': circuit,\n",
    "        'clean_avg': clean,\n",
    "        'dirty_avg': dirty,\n",
    "        'clean_advantage': dirty - clean\n",
    "    })\n",
    "\n",
    "grid_side_df = pd.DataFrame(circuit_grid_side).sort_values('clean_advantage', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "colors = ['green' if x > 0 else 'red' for x in grid_side_df['clean_advantage']]\n",
    "ax.barh(grid_side_df['circuit'], grid_side_df['clean_advantage'],\n",
    "        color=colors, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Clean Side Advantage (positions)', fontweight='bold')\n",
    "ax.set_ylabel('Circuit', fontweight='bold')\n",
    "ax.set_title('Grid Side Effect by Circuit', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 circuits with strongest clean side advantage:\")\n",
    "print(grid_side_df.head(5)[['circuit', 'clean_advantage']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Team Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance metrics\n",
    "team_stats = []\n",
    "\n",
    "for team in df_finished['TeamName'].unique():\n",
    "    team_data = df_finished[df_finished['TeamName'] == team]\n",
    "    team_all = df_all[df_all['TeamName'] == team]\n",
    "    \n",
    "    team_stats.append({\n",
    "        'team': team,\n",
    "        'races': len(team_all),\n",
    "        'avg_grid': team_data['GridPosition'].mean(),\n",
    "        'avg_finish': team_data['Position'].mean(),\n",
    "        'performance_delta': team_data['GridPosition'].mean() - team_data['Position'].mean(),\n",
    "        'consistency': team_data['Position'].std(),\n",
    "        'dnf_rate': (team_all['is_dnf'].sum() / len(team_all) * 100),\n",
    "        'win_rate': ((team_data['Position'] == 1).sum() / len(team_all) * 100),\n",
    "        'podium_rate': ((team_data['Position'] <= 3).sum() / len(team_all) * 100),\n",
    "        'points_rate': ((team_data['Position'] <= 10).sum() / len(team_all) * 100)\n",
    "    })\n",
    "\n",
    "team_df = pd.DataFrame(team_stats).sort_values('performance_delta', ascending=False)\n",
    "print(\"Team Performance Analysis:\")\n",
    "print(team_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance scatter plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "def get_tier_color(avg_finish):\n",
    "    if avg_finish <= 5:\n",
    "        return 'gold'\n",
    "    elif avg_finish <= 10:\n",
    "        return 'silver'\n",
    "    else:\n",
    "        return '#cd7f32'\n",
    "\n",
    "colors = [get_tier_color(x) for x in team_df['avg_finish']]\n",
    "scatter = ax.scatter(team_df['avg_grid'], team_df['avg_finish'],\n",
    "                     s=team_df['races'] * 2, alpha=0.6,\n",
    "                     c=colors, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "ax.plot([0, 20], [0, 20], 'r--', alpha=0.5, linewidth=2, label='No change')\n",
    "\n",
    "for idx, row in team_df.iterrows():\n",
    "    ax.annotate(row['team'], (row['avg_grid'], row['avg_finish']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Average Grid Position', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Team Performance: Grid vs Finish', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.text(2, 18, 'Over-performing', fontsize=11, style='italic', color='green')\n",
    "ax.text(18, 2, 'Under-performing', fontsize=11, style='italic', color='red')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position change by team\n",
    "sorted_teams = team_df.sort_values('performance_delta', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "colors_perf = ['green' if x > 0 else 'red' for x in sorted_teams['performance_delta']]\n",
    "ax.barh(sorted_teams['team'], sorted_teams['performance_delta'],\n",
    "        color=colors_perf, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Average Position Change', fontweight='bold')\n",
    "ax.set_ylabel('Team', fontweight='bold')\n",
    "ax.set_title('Team Performance Efficiency', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 over-performing teams:\")\n",
    "print(team_df.head(5)[['team', 'performance_delta', 'avg_grid', 'avg_finish']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance over time\n",
    "major_teams = ['Mercedes', 'Red Bull Racing', 'Ferrari', 'McLaren', 'Alpine']\n",
    "available_major = [t for t in major_teams if t in df_finished['TeamName'].values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for team in available_major:\n",
    "    team_yearly = []\n",
    "    for year in sorted(df_finished['year'].unique()):\n",
    "        year_team = df_finished[(df_finished['year'] == year) & \n",
    "                                (df_finished['TeamName'] == team)]\n",
    "        if len(year_team) > 0:\n",
    "            team_yearly.append({'year': year, 'avg_finish': year_team['Position'].mean()})\n",
    "    \n",
    "    if len(team_yearly) > 0:\n",
    "        team_yearly_df = pd.DataFrame(team_yearly)\n",
    "        ax.plot(team_yearly_df['year'], team_yearly_df['avg_finish'],\n",
    "                marker='o', linewidth=2, markersize=6, label=team)\n",
    "\n",
    "ax.set_xlabel('Year', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Team Performance Evolution (2018-2024)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive statistical testing\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Correlation tests\n",
    "print(\"\\n1. CORRELATION TESTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "pearson_r, pearson_p = pearsonr(df_finished['GridPosition'], df_finished['Position'])\n",
    "print(f\"\\nPearson correlation:\")\n",
    "print(f\"  r = {pearson_r:.4f}\")\n",
    "print(f\"  p-value = {pearson_p:.6f}\")\n",
    "print(f\"  R² = {pearson_r**2:.4f} ({pearson_r**2*100:.1f}% variance explained)\")\n",
    "\n",
    "results.append({'test': 'Pearson Correlation', 'statistic': pearson_r, \n",
    "                'p_value': pearson_p, 'significant': pearson_p < 0.05})\n",
    "\n",
    "spearman_r, spearman_p = spearmanr(df_finished['GridPosition'], df_finished['Position'])\n",
    "print(f\"\\nSpearman rank correlation:\")\n",
    "print(f\"  ρ = {spearman_r:.4f}\")\n",
    "print(f\"  p-value = {spearman_p:.6f}\")\n",
    "\n",
    "results.append({'test': 'Spearman Correlation', 'statistic': spearman_r,\n",
    "                'p_value': spearman_p, 'significant': spearman_p < 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ANOVA\n",
    "print(\"\\n2. ONE-WAY ANOVA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "grid_groups = [df_finished[df_finished['GridPosition'] == i]['Position'].values \n",
    "               for i in range(1, 21) if len(df_finished[df_finished['GridPosition'] == i]) > 0]\n",
    "\n",
    "f_stat, anova_p = f_oneway(*grid_groups)\n",
    "print(f\"\\nOne-way ANOVA (finish position by grid position):\")\n",
    "print(f\"  F-statistic = {f_stat:.4f}\")\n",
    "print(f\"  p-value = {anova_p:.6f}\")\n",
    "\n",
    "if anova_p < 0.05:\n",
    "    print(f\"  Result: Finish positions significantly differ across grid positions\")\n",
    "\n",
    "results.append({'test': 'One-way ANOVA', 'statistic': f_stat,\n",
    "                'p_value': anova_p, 'significant': anova_p < 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Two-way ANOVA\n",
    "print(\"\\n3. TWO-WAY ANOVA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "top_circuits = df_finished['circuit'].value_counts().head(10).index\n",
    "analysis_df = df_finished[\n",
    "    (df_finished['circuit'].isin(top_circuits)) & \n",
    "    (df_finished['GridPosition'] <= 10)\n",
    "].copy()\n",
    "\n",
    "model = ols('Position ~ C(GridPosition) + C(circuit) + C(GridPosition):C(circuit)', \n",
    "            data=analysis_df).fit()\n",
    "anova_results = anova_lm(model, typ=2)\n",
    "\n",
    "print(\"\\nTwo-way ANOVA results:\")\n",
    "print(anova_results)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "for factor in ['C(GridPosition)', 'C(circuit)', 'C(GridPosition):C(circuit)']:\n",
    "    if factor in anova_results.index:\n",
    "        p_val = anova_results.loc[factor, 'PR(>F)']\n",
    "        if p_val < 0.05:\n",
    "            print(f\"  {factor}: Significant effect (p = {p_val:.6f})\")\n",
    "        else:\n",
    "            print(f\"  {factor}: No significant effect (p = {p_val:.6f})\")\n",
    "\n",
    "if 'C(GridPosition):C(circuit)' in anova_results.index:\n",
    "    results.append({'test': 'Two-way ANOVA (Interaction)', \n",
    "                    'statistic': anova_results.loc['C(GridPosition):C(circuit)', 'F'],\n",
    "                    'p_value': anova_results.loc['C(GridPosition):C(circuit)', 'PR(>F)'],\n",
    "                    'significant': anova_results.loc['C(GridPosition):C(circuit)', 'PR(>F)'] < 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tukey HSD post-hoc test\n",
    "print(\"\\n4. TUKEY HSD POST-HOC TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "tukey_df = df_finished[df_finished['GridPosition'] <= 5].copy()\n",
    "tukey_result = pairwise_tukeyhsd(\n",
    "    endog=tukey_df['Position'],\n",
    "    groups=tukey_df['GridPosition'],\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nTukey HSD results (P1-P5):\")\n",
    "print(tukey_result)\n",
    "\n",
    "print(\"\\nSignificant pairwise differences:\")\n",
    "tukey_summary = tukey_result.summary()\n",
    "sig_pairs = []\n",
    "for i in range(1, len(tukey_summary.data)):\n",
    "    row = tukey_summary.data[i]\n",
    "    if row[5] == 'True':\n",
    "        sig_pairs.append(f\"P{row[0]} vs P{row[1]}\")\n",
    "\n",
    "if sig_pairs:\n",
    "    for pair in sig_pairs[:10]:\n",
    "        print(f\"  {pair}\")\n",
    "else:\n",
    "    print(\"  No significant pairwise differences found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Chi-square tests\n",
    "print(\"\\n5. CHI-SQUARE TESTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "pole_data = df_finished[df_finished['GridPosition'] == 1]\n",
    "pole_wins = (pole_data['Position'] == 1).sum()\n",
    "pole_non_wins = len(pole_data) - pole_wins\n",
    "expected_wins = len(pole_data) / 20\n",
    "expected_non_wins = len(pole_data) - expected_wins\n",
    "\n",
    "chi2_pole, p_pole = stats.chisquare([pole_wins, pole_non_wins], \n",
    "                                     [expected_wins, expected_non_wins])\n",
    "\n",
    "print(f\"\\nPole position advantage vs random chance:\")\n",
    "print(f\"  Observed wins: {pole_wins}\")\n",
    "print(f\"  Expected wins (random): {expected_wins:.1f}\")\n",
    "print(f\"  χ² = {chi2_pole:.4f}\")\n",
    "print(f\"  p-value = {p_pole:.6f}\")\n",
    "\n",
    "if p_pole < 0.05:\n",
    "    print(f\"  Result: Pole advantage is statistically significant\")\n",
    "\n",
    "results.append({'test': 'Chi-Square (Pole)', 'statistic': chi2_pole,\n",
    "                'p_value': p_pole, 'significant': p_pole < 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Normality test\n",
    "print(\"\\n6. NORMALITY TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "ks_stat, ks_p = kstest(df_finished['position_change'], 'norm',\n",
    "                       args=(df_finished['position_change'].mean(),\n",
    "                             df_finished['position_change'].std()))\n",
    "\n",
    "print(f\"\\nKolmogorov-Smirnov test (position change normality):\")\n",
    "print(f\"  KS-statistic = {ks_stat:.4f}\")\n",
    "print(f\"  p-value = {ks_p:.6f}\")\n",
    "\n",
    "if ks_p < 0.05:\n",
    "    print(f\"  Result: Position change is NOT normally distributed\")\n",
    "else:\n",
    "    print(f\"  Result: Position change follows normal distribution\")\n",
    "\n",
    "results.append({'test': 'K-S Normality', 'statistic': ks_stat,\n",
    "                'p_value': ks_p, 'significant': ks_p < 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Variance equality test\n",
    "print(\"\\n7. VARIANCE EQUALITY TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "variance_groups = [df_finished[df_finished['GridPosition'] == i]['position_change'].values \n",
    "                   for i in range(1, 6)]\n",
    "\n",
    "lev_stat, lev_p = levene(*variance_groups)\n",
    "\n",
    "print(f\"\\nLevene's test (variance equality across P1-P5):\")\n",
    "print(f\"  Statistic = {lev_stat:.4f}\")\n",
    "print(f\"  p-value = {lev_p:.6f}\")\n",
    "\n",
    "if lev_p < 0.05:\n",
    "    print(f\"  Result: Variances are NOT equal across grid positions\")\n",
    "else:\n",
    "    print(f\"  Result: Variances are equal\")\n",
    "\n",
    "results.append({'test': \"Levene's Test\", 'statistic': lev_stat,\n",
    "                'p_value': lev_p, 'significant': lev_p < 0.05})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL TEST SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nSignificant results: {results_df['significant'].sum()}/{len(results_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Momentum Analysis\n",
    "\n",
    "Investigating team and driver hot streaks and recent form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling 3-race performance\n",
    "df_all_sorted = df_all.sort_values(['FullName', 'year', 'round']).copy()\n",
    "\n",
    "df_all_sorted['rolling_3_finish'] = df_all_sorted.groupby('FullName')['Position'].transform(\n",
    "    lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "df_all_sorted['prev_race_finish'] = df_all_sorted.groupby('FullName')['Position'].shift(1)\n",
    "df_all_sorted['momentum'] = df_all_sorted['prev_race_finish'] - df_all_sorted['Position']\n",
    "\n",
    "driver_race_counts = df_all_sorted.groupby('FullName').size()\n",
    "drivers_with_data = driver_race_counts[driver_race_counts >= 10].index\n",
    "momentum_df = df_all_sorted[df_all_sorted['FullName'].isin(drivers_with_data)].copy()\n",
    "\n",
    "print(f\"Momentum Analysis Summary:\")\n",
    "print(f\"Drivers analyzed: {len(drivers_with_data)}\")\n",
    "print(f\"Total races: {len(momentum_df)}\")\n",
    "print(f\"\\nOverall momentum statistics:\")\n",
    "print(momentum_df['momentum'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictive power of recent form\n",
    "momentum_test = momentum_df[momentum_df['rolling_3_finish'].notna()].copy()\n",
    "corr_rolling, p_rolling = pearsonr(momentum_test['rolling_3_finish'], momentum_test['Position'])\n",
    "\n",
    "print(f\"\\nPredictive Power of Recent Form:\")\n",
    "print(f\"Rolling 3-race average vs current finish:\")\n",
    "print(f\"  Correlation: {corr_rolling:.4f}\")\n",
    "print(f\"  P-value: {p_rolling:.6f}\")\n",
    "\n",
    "if p_rolling < 0.05:\n",
    "    print(f\"  Result: Recent form significantly predicts current performance\")\n",
    "else:\n",
    "    print(f\"  Result: Recent form does not significantly predict performance\")\n",
    "\n",
    "# Momentum prediction\n",
    "momentum_next = momentum_df.copy()\n",
    "momentum_next['next_race_finish'] = momentum_next.groupby('FullName')['Position'].shift(-1)\n",
    "momentum_pred = momentum_next[momentum_next['next_race_finish'].notna()]\n",
    "\n",
    "if len(momentum_pred) > 0:\n",
    "    corr_momentum, p_momentum = pearsonr(momentum_pred['momentum'].fillna(0),\n",
    "                                          momentum_pred['next_race_finish'])\n",
    "    print(f\"\\nMomentum vs next race:\")\n",
    "    print(f\"  Correlation: {corr_momentum:.4f}\")\n",
    "    print(f\"  P-value: {p_momentum:.6f}\")\n",
    "    \n",
    "    if abs(corr_momentum) > 0.1:\n",
    "        print(f\"  Result: Momentum shows weak predictive signal\")\n",
    "    else:\n",
    "        print(f\"  Result: Momentum does not predict next race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hot streak identification\n",
    "def identify_streaks(driver_data):\n",
    "    streaks = []\n",
    "    current_streak = 0\n",
    "    \n",
    "    for idx in range(1, len(driver_data)):\n",
    "        prev_pos = driver_data.iloc[idx-1]['Position']\n",
    "        curr_pos = driver_data.iloc[idx]['Position']\n",
    "        \n",
    "        if curr_pos < prev_pos:\n",
    "            current_streak += 1\n",
    "        else:\n",
    "            if current_streak >= 3:\n",
    "                streaks.append(current_streak)\n",
    "            current_streak = 0\n",
    "    \n",
    "    if current_streak >= 3:\n",
    "        streaks.append(current_streak)\n",
    "    return streaks\n",
    "\n",
    "all_streaks = []\n",
    "for driver in drivers_with_data:\n",
    "    driver_data = df_all_sorted[df_all_sorted['FullName'] == driver].copy()\n",
    "    all_streaks.extend(identify_streaks(driver_data))\n",
    "\n",
    "print(f\"\\nHot Streak Analysis:\")\n",
    "print(f\"Total hot streaks (3+ improving races): {len(all_streaks)}\")\n",
    "\n",
    "if len(all_streaks) > 0:\n",
    "    print(f\"Average streak length: {np.mean(all_streaks):.1f} races\")\n",
    "    print(f\"Longest streak: {max(all_streaks)} races\")\n",
    "    print(f\"\\nStreak distribution:\")\n",
    "    for length in sorted(set(all_streaks)):\n",
    "        count = all_streaks.count(length)\n",
    "        print(f\"  {length} races: {count} occurrences\")\n",
    "else:\n",
    "    print(\"No significant hot streaks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Qualifying vs Race Performance\n",
    "\n",
    "Identifying qualifying specialists vs race performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate driver profiles\n",
    "driver_performance = []\n",
    "\n",
    "for driver in drivers_with_data:\n",
    "    driver_data = df_finished[df_finished['FullName'] == driver].copy()\n",
    "    \n",
    "    if len(driver_data) < 5:\n",
    "        continue\n",
    "    \n",
    "    avg_change = driver_data['position_change'].mean()\n",
    "    \n",
    "    driver_performance.append({\n",
    "        'driver': driver,\n",
    "        'races': len(driver_data),\n",
    "        'avg_grid': driver_data['GridPosition'].mean(),\n",
    "        'avg_finish': driver_data['Position'].mean(),\n",
    "        'avg_position_change': avg_change,\n",
    "        'qualifying_vs_race': 'Race Performer' if avg_change > 0.5 else 'Qualifying Specialist' if avg_change < -0.5 else 'Balanced'\n",
    "    })\n",
    "\n",
    "perf_df = pd.DataFrame(driver_performance).sort_values('avg_position_change', ascending=False)\n",
    "\n",
    "print(\"Driver Performance Profiles:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTop 5 Race Performers (gain positions):\")\n",
    "for idx, row in perf_df.head(5).iterrows():\n",
    "    print(f\"  {row['driver']}: +{row['avg_position_change']:.2f} positions/race\")\n",
    "\n",
    "print(\"\\nTop 5 Qualifying Specialists (lose positions):\")\n",
    "for idx, row in perf_df.tail(5).iterrows():\n",
    "    print(f\"  {row['driver']}: {row['avg_position_change']:.2f} positions/race\")\n",
    "\n",
    "print(f\"\\nPerformance Type Distribution:\")\n",
    "print(perf_df['qualifying_vs_race'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize driver profiles\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "colors_type = perf_df['qualifying_vs_race'].map({\n",
    "    'Race Performer': 'green',\n",
    "    'Balanced': 'gray',\n",
    "    'Qualifying Specialist': 'red'\n",
    "})\n",
    "\n",
    "ax.scatter(perf_df['avg_grid'], perf_df['avg_finish'],\n",
    "           s=perf_df['races'] * 10, alpha=0.6,\n",
    "           c=colors_type, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "ax.plot([0, 20], [0, 20], 'k--', alpha=0.5, linewidth=2, label='No change')\n",
    "\n",
    "for idx, row in perf_df.iterrows():\n",
    "    if abs(row['avg_position_change']) > 1.5:\n",
    "        ax.annotate(row['driver'], (row['avg_grid'], row['avg_finish']),\n",
    "                    fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Average Grid Position', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Driver Profile: Qualifying vs Race Performance', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', edgecolor='black', label='Race Performer'),\n",
    "    Patch(facecolor='gray', edgecolor='black', label='Balanced'),\n",
    "    Patch(facecolor='red', edgecolor='black', label='Qualifying Specialist')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "ax.text(2, 18, 'Strong race pace', fontsize=11, style='italic', color='green')\n",
    "ax.text(18, 2, 'Over-qualify car', fontsize=11, style='italic', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Weather Impact Analysis\n",
    "\n",
    "Investigating effects of weather conditions on race outcomes (if data available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check weather data availability\n",
    "weather_columns = ['AirTemp', 'Humidity', 'Pressure', 'Rainfall', 'TrackTemp', 'WindDirection', 'WindSpeed']\n",
    "available_weather = [col for col in weather_columns if col in df_all.columns]\n",
    "\n",
    "print(f\"Weather Data Availability:\")\n",
    "print(f\"Columns in dataset: {df_all.columns.tolist()[:20]}...\")\n",
    "\n",
    "if len(available_weather) > 0:\n",
    "    print(f\"\\nAvailable weather features: {available_weather}\")\n",
    "    for col in available_weather:\n",
    "        non_null = df_all[col].notna().sum()\n",
    "        pct_available = (non_null / len(df_all)) * 100\n",
    "        print(f\"  {col}: {non_null}/{len(df_all)} ({pct_available:.1f}% coverage)\")\n",
    "else:\n",
    "    print(\"\\nNo weather data available in dataset.\")\n",
    "    print(\"Consider collecting weather data from FastF1 API or external sources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather impact analysis (if data available)\n",
    "if len(available_weather) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WEATHER IMPACT ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if 'Rainfall' in available_weather:\n",
    "        rain_data = df_finished[df_finished['Rainfall'].notna()].copy()\n",
    "        \n",
    "        if len(rain_data) > 0:\n",
    "            rain_data['is_wet'] = rain_data['Rainfall'] > 0\n",
    "            wet_races = rain_data[rain_data['is_wet'] == True]\n",
    "            dry_races = rain_data[rain_data['is_wet'] == False]\n",
    "            \n",
    "            print(\"\\nRain Impact on Position Changes:\")\n",
    "            print(f\"Wet races: {len(wet_races)}\")\n",
    "            print(f\"Dry races: {len(dry_races)}\")\n",
    "            \n",
    "            if len(wet_races) > 0 and len(dry_races) > 0:\n",
    "                wet_var = wet_races['position_change'].var()\n",
    "                dry_var = dry_races['position_change'].var()\n",
    "                \n",
    "                print(f\"\\nPosition change variance:\")\n",
    "                print(f\"  Wet: {wet_var:.2f}\")\n",
    "                print(f\"  Dry: {dry_var:.2f}\")\n",
    "                print(f\"  Difference: {wet_var - dry_var:+.2f}\")\n",
    "                \n",
    "                lev_stat, lev_p = levene(wet_races['position_change'], dry_races['position_change'])\n",
    "                print(f\"\\nLevene's test: p = {lev_p:.6f}\")\n",
    "                if lev_p < 0.05:\n",
    "                    print(f\"  Result: Rain significantly affects race unpredictability\")\n",
    "    \n",
    "    if 'TrackTemp' in available_weather:\n",
    "        temp_data = df_finished[df_finished['TrackTemp'].notna()].copy()\n",
    "        \n",
    "        if len(temp_data) > 20:\n",
    "            temp_corr, temp_p = pearsonr(temp_data['TrackTemp'], \n",
    "                                         temp_data['position_change'].abs())\n",
    "            \n",
    "            print(f\"\\nTrack Temperature Impact:\")\n",
    "            print(f\"  Correlation: {temp_corr:.4f}\")\n",
    "            print(f\"  P-value: {temp_p:.6f}\")\n",
    "else:\n",
    "    print(\"\\n[Weather analysis skipped - no data available]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
