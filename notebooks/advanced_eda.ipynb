{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Advanced EDA & Statistical Testing\n",
    "\n",
    "Deep statistical analysis of grid position effects, team performance, and validation testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, ttest_ind, pearsonr, spearmanr\n",
    "from scipy.stats import f_oneway, chi2_contingency, kstest, levene\n",
    "from pathlib import Path\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path('../data/processed/processed_race_data.csv')\n",
    "\n",
    "df_all = pd.read_csv(data_path)\n",
    "df_finished = df_all[df_all['completed_race'] == True].copy()\n",
    "\n",
    "print(f\"Total records: {len(df_all):,}\")\n",
    "print(f\"Finished races: {len(df_finished):,}\")\n",
    "print(f\"Years: {sorted(df_all['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Grid Side Analysis (Clean vs Dirty)\n",
    "\n",
    "Odd positions (P1, P3, P5...) are on the racing line (clean). Even positions (P2, P4, P6...) are off-line (dirty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add grid side classification\n",
    "df_finished['grid_side'] = df_finished['GridPosition'].apply(\n",
    "    lambda x: 'clean' if x % 2 == 1 else 'dirty'\n",
    ")\n",
    "\n",
    "print(\"Grid side distribution:\")\n",
    "print(df_finished['grid_side'].value_counts())\n",
    "\n",
    "# Overall comparison\n",
    "clean_stats = df_finished[df_finished['grid_side'] == 'clean']['position_change'].describe()\n",
    "dirty_stats = df_finished[df_finished['grid_side'] == 'dirty']['position_change'].describe()\n",
    "\n",
    "print(\"\\nPosition change by grid side:\")\n",
    "print(\"\\nClean side (odd positions):\")\n",
    "print(clean_stats)\n",
    "print(\"\\nDirty side (even positions):\")\n",
    "print(dirty_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired comparison - compare each row of grid\n",
    "paired_analysis = []\n",
    "\n",
    "for row in range(1, 11):  # Rows 1-10 (P1/P2, P3/P4, ... P19/P20)\n",
    "    clean_pos = row * 2 - 1  # Odd position\n",
    "    dirty_pos = row * 2      # Even position\n",
    "    \n",
    "    clean_data = df_finished[df_finished['GridPosition'] == clean_pos]\n",
    "    dirty_data = df_finished[df_finished['GridPosition'] == dirty_pos]\n",
    "    \n",
    "    if len(clean_data) > 0 and len(dirty_data) > 0:\n",
    "        clean_avg_change = clean_data['position_change'].mean()\n",
    "        dirty_avg_change = dirty_data['position_change'].mean()\n",
    "        \n",
    "        clean_avg_finish = clean_data['Position'].mean()\n",
    "        dirty_avg_finish = dirty_data['Position'].mean()\n",
    "        \n",
    "        # Advantage is how much better clean side performs\n",
    "        advantage = dirty_avg_change - clean_avg_change\n",
    "        \n",
    "        paired_analysis.append({\n",
    "            'row': row,\n",
    "            'clean_pos': clean_pos,\n",
    "            'dirty_pos': dirty_pos,\n",
    "            'clean_avg_change': clean_avg_change,\n",
    "            'dirty_avg_change': dirty_avg_change,\n",
    "            'clean_advantage': advantage,\n",
    "            'clean_avg_finish': clean_avg_finish,\n",
    "            'dirty_avg_finish': dirty_avg_finish\n",
    "        })\n",
    "\n",
    "paired_df = pd.DataFrame(paired_analysis)\n",
    "\n",
    "print(\"\\nPaired grid row analysis:\")\n",
    "print(paired_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nOverall clean side advantage: {paired_df['clean_advantage'].mean():.3f} positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired t-test\n",
    "clean_changes = df_finished[df_finished['grid_side'] == 'clean']['position_change'].values\n",
    "dirty_changes = df_finished[df_finished['grid_side'] == 'dirty']['position_change'].values\n",
    "\n",
    "# Since not truly paired (different sample sizes), use independent t-test\n",
    "t_stat, p_value = ttest_ind(clean_changes, dirty_changes)\n",
    "\n",
    "print(f\"\\nStatistical test (clean vs dirty):\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Clean side advantage is statistically significant\")\n",
    "else:\n",
    "    print(\"Result: No statistically significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid side advantage by circuit\n",
    "circuit_grid_side = []\n",
    "\n",
    "for circuit in df_finished['circuit'].unique():\n",
    "    circuit_data = df_finished[df_finished['circuit'] == circuit]\n",
    "    \n",
    "    clean = circuit_data[circuit_data['grid_side'] == 'clean']['position_change'].mean()\n",
    "    dirty = circuit_data[circuit_data['grid_side'] == 'dirty']['position_change'].mean()\n",
    "    \n",
    "    advantage = dirty - clean\n",
    "    \n",
    "    circuit_grid_side.append({\n",
    "        'circuit': circuit,\n",
    "        'clean_avg': clean,\n",
    "        'dirty_avg': dirty,\n",
    "        'clean_advantage': advantage\n",
    "    })\n",
    "\n",
    "grid_side_df = pd.DataFrame(circuit_grid_side).sort_values('clean_advantage', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "colors = ['green' if x > 0 else 'red' for x in grid_side_df['clean_advantage']]\n",
    "ax.barh(grid_side_df['circuit'], grid_side_df['clean_advantage'],\n",
    "        color=colors, edgecolor='black', alpha=0.7)\n",
    "\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Clean Side Advantage (positions)', fontweight='bold')\n",
    "ax.set_ylabel('Circuit', fontweight='bold')\n",
    "ax.set_title('Grid Side Effect by Circuit', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 circuits with strongest clean side advantage:\")\n",
    "print(grid_side_df.head(5)[['circuit', 'clean_advantage']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Team Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance metrics\n",
    "team_stats = []\n",
    "\n",
    "for team in df_finished['TeamName'].unique():\n",
    "    team_data = df_finished[df_finished['TeamName'] == team]\n",
    "    team_all = df_all[df_all['TeamName'] == team]\n",
    "    \n",
    "    avg_grid = team_data['GridPosition'].mean()\n",
    "    avg_finish = team_data['Position'].mean()\n",
    "    performance_delta = avg_grid - avg_finish  # Positive = gaining positions\n",
    "    \n",
    "    consistency = team_data['Position'].std()\n",
    "    dnf_rate = (team_all['is_dnf'].sum() / len(team_all) * 100)\n",
    "    \n",
    "    num_races = len(team_all)\n",
    "    wins = (team_data['Position'] == 1).sum()\n",
    "    podiums = (team_data['Position'] <= 3).sum()\n",
    "    points_finishes = (team_data['Position'] <= 10).sum()\n",
    "    \n",
    "    win_rate = (wins / num_races * 100) if num_races > 0 else 0\n",
    "    podium_rate = (podiums / num_races * 100) if num_races > 0 else 0\n",
    "    points_rate = (points_finishes / num_races * 100) if num_races > 0 else 0\n",
    "    \n",
    "    team_stats.append({\n",
    "        'team': team,\n",
    "        'races': num_races,\n",
    "        'avg_grid': avg_grid,\n",
    "        'avg_finish': avg_finish,\n",
    "        'performance_delta': performance_delta,\n",
    "        'consistency': consistency,\n",
    "        'dnf_rate': dnf_rate,\n",
    "        'win_rate': win_rate,\n",
    "        'podium_rate': podium_rate,\n",
    "        'points_rate': points_rate\n",
    "    })\n",
    "\n",
    "team_df = pd.DataFrame(team_stats).sort_values('performance_delta', ascending=False)\n",
    "\n",
    "print(\"Team Performance Analysis:\")\n",
    "print(team_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance scatter plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Color code by performance tier\n",
    "def get_tier_color(avg_finish):\n",
    "    if avg_finish <= 5:\n",
    "        return 'gold'\n",
    "    elif avg_finish <= 10:\n",
    "        return 'silver'\n",
    "    else:\n",
    "        return '#cd7f32'\n",
    "\n",
    "colors = [get_tier_color(x) for x in team_df['avg_finish']]\n",
    "\n",
    "scatter = ax.scatter(team_df['avg_grid'], team_df['avg_finish'],\n",
    "                     s=team_df['races'] * 2, alpha=0.6,\n",
    "                     c=colors, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "# Reference line (no change)\n",
    "ax.plot([0, 20], [0, 20], 'r--', alpha=0.5, linewidth=2, label='No change')\n",
    "\n",
    "# Label teams\n",
    "for idx, row in team_df.iterrows():\n",
    "    ax.annotate(row['team'], (row['avg_grid'], row['avg_finish']),\n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "\n",
    "ax.set_xlabel('Average Grid Position', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Team Performance: Grid vs Finish', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(2, 18, 'Over-performing', fontsize=11, style='italic', color='green')\n",
    "ax.text(18, 2, 'Under-performing', fontsize=11, style='italic', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position change by team\n",
    "sorted_teams = team_df.sort_values('performance_delta', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "colors_perf = ['green' if x > 0 else 'red' for x in sorted_teams['performance_delta']]\n",
    "ax.barh(sorted_teams['team'], sorted_teams['performance_delta'],\n",
    "        color=colors_perf, edgecolor='black', alpha=0.7)\n",
    "\n",
    "ax.axvline(0, color='black', linestyle='-', linewidth=1)\n",
    "ax.set_xlabel('Average Position Change', fontweight='bold')\n",
    "ax.set_ylabel('Team', fontweight='bold')\n",
    "ax.set_title('Team Performance Efficiency', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 over-performing teams:\")\n",
    "print(team_df.head(5)[['team', 'performance_delta', 'avg_grid', 'avg_finish']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team performance over time (major teams)\n",
    "major_teams = ['Mercedes', 'Red Bull Racing', 'Ferrari', 'McLaren', 'Alpine']\n",
    "available_major = [t for t in major_teams if t in df_finished['TeamName'].values]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for team in available_major:\n",
    "    team_yearly = []\n",
    "    \n",
    "    for year in sorted(df_finished['year'].unique()):\n",
    "        year_team = df_finished[(df_finished['year'] == year) & \n",
    "                                (df_finished['TeamName'] == team)]\n",
    "        if len(year_team) > 0:\n",
    "            avg_finish = year_team['Position'].mean()\n",
    "            team_yearly.append({'year': year, 'avg_finish': avg_finish})\n",
    "    \n",
    "    if len(team_yearly) > 0:\n",
    "        team_yearly_df = pd.DataFrame(team_yearly)\n",
    "        ax.plot(team_yearly_df['year'], team_yearly_df['avg_finish'],\n",
    "                marker='o', linewidth=2, markersize=6, label=team)\n",
    "\n",
    "ax.set_xlabel('Year', fontweight='bold')\n",
    "ax.set_ylabel('Average Finish Position', fontweight='bold')\n",
    "ax.set_title('Team Performance Evolution (2018-2024)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "ax.invert_yaxis()  # Lower is better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive statistical testing\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Correlation tests\n",
    "print(\"\\n1. CORRELATION TESTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "pearson_r, pearson_p = pearsonr(df_finished['GridPosition'], df_finished['Position'])\n",
    "print(f\"\\nPearson correlation:\")\n",
    "print(f\"  r = {pearson_r:.4f}\")\n",
    "print(f\"  p-value = {pearson_p:.6f}\")\n",
    "print(f\"  R² = {pearson_r**2:.4f} ({pearson_r**2*100:.1f}% variance explained)\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'Pearson Correlation',\n",
    "    'statistic': pearson_r,\n",
    "    'p_value': pearson_p,\n",
    "    'significant': pearson_p < 0.05\n",
    "})\n",
    "\n",
    "spearman_r, spearman_p = spearmanr(df_finished['GridPosition'], df_finished['Position'])\n",
    "print(f\"\\nSpearman rank correlation:\")\n",
    "print(f\"  ρ = {spearman_r:.4f}\")\n",
    "print(f\"  p-value = {spearman_p:.6f}\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'Spearman Correlation',\n",
    "    'statistic': spearman_r,\n",
    "    'p_value': spearman_p,\n",
    "    'significant': spearman_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ANOVA - finish positions different across grid positions?\n",
    "print(\"\\n2. ANOVA TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "grid_groups = [df_finished[df_finished['GridPosition'] == i]['Position'].values \n",
    "               for i in range(1, 21) if len(df_finished[df_finished['GridPosition'] == i]) > 0]\n",
    "\n",
    "f_stat, anova_p = f_oneway(*grid_groups)\n",
    "\n",
    "print(f\"\\nOne-way ANOVA (finish position by grid position):\")\n",
    "print(f\"  F-statistic = {f_stat:.4f}\")\n",
    "print(f\"  p-value = {anova_p:.6f}\")\n",
    "\n",
    "if anova_p < 0.05:\n",
    "    print(f\"  Result: Finish positions significantly differ across grid positions\")\n",
    "else:\n",
    "    print(f\"  Result: No significant difference\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'ANOVA',\n",
    "    'statistic': f_stat,\n",
    "    'p_value': anova_p,\n",
    "    'significant': anova_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Chi-square tests\n",
    "print(\"\\n3. CHI-SQUARE TESTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Pole position advantage\n",
    "pole_data = df_finished[df_finished['GridPosition'] == 1]\n",
    "pole_wins = (pole_data['Position'] == 1).sum()\n",
    "pole_non_wins = len(pole_data) - pole_wins\n",
    "\n",
    "# Expected if random (1/20 chance)\n",
    "expected_wins = len(pole_data) / 20\n",
    "expected_non_wins = len(pole_data) - expected_wins\n",
    "\n",
    "chi2_pole, p_pole = stats.chisquare([pole_wins, pole_non_wins], \n",
    "                                     [expected_wins, expected_non_wins])\n",
    "\n",
    "print(f\"\\nPole position advantage vs random chance:\")\n",
    "print(f\"  Observed wins: {pole_wins}\")\n",
    "print(f\"  Expected wins (random): {expected_wins:.1f}\")\n",
    "print(f\"  χ² = {chi2_pole:.4f}\")\n",
    "print(f\"  p-value = {p_pole:.6f}\")\n",
    "\n",
    "if p_pole < 0.05:\n",
    "    print(f\"  Result: Pole advantage is statistically significant\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'Chi-Square (Pole)',\n",
    "    'statistic': chi2_pole,\n",
    "    'p_value': p_pole,\n",
    "    'significant': p_pole < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Normality test on position change\n",
    "print(\"\\n4. NORMALITY TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "ks_stat, ks_p = kstest(df_finished['position_change'], 'norm',\n",
    "                       args=(df_finished['position_change'].mean(),\n",
    "                             df_finished['position_change'].std()))\n",
    "\n",
    "print(f\"\\nKolmogorov-Smirnov test (position change normality):\")\n",
    "print(f\"  KS-statistic = {ks_stat:.4f}\")\n",
    "print(f\"  p-value = {ks_p:.6f}\")\n",
    "\n",
    "if ks_p < 0.05:\n",
    "    print(f\"  Result: Position change is NOT normally distributed\")\n",
    "else:\n",
    "    print(f\"  Result: Position change follows normal distribution\")\n",
    "\n",
    "results.append({\n",
    "    'test': 'K-S Normality',\n",
    "    'statistic': ks_stat,\n",
    "    'p_value': ks_p,\n",
    "    'significant': ks_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Variance equality test\n",
    "print(\"\\n5. VARIANCE EQUALITY TEST\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Compare variance across first 5 grid positions\n",
    "variance_groups = [df_finished[df_finished['GridPosition'] == i]['position_change'].values \n",
    "                   for i in range(1, 6)]\n",
    "\n",
    "lev_stat, lev_p = levene(*variance_groups)\n",
    "\n",
    "print(f\"\\nLevene's test (variance equality across P1-P5):\")\n",
    "print(f\"  Statistic = {lev_stat:.4f}\")\n",
    "print(f\"  p-value = {lev_p:.6f}\")\n",
    "\n",
    "if lev_p < 0.05:\n",
    "    print(f\"  Result: Variances are NOT equal across grid positions\")\n",
    "else:\n",
    "    print(f\"  Result: Variances are equal\")\n",
    "\n",
    "results.append({\n",
    "    'test': \"Levene's Test\",\n",
    "    'statistic': lev_stat,\n",
    "    'p_value': lev_p,\n",
    "    'significant': lev_p < 0.05\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL TEST SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nSignificant results: {results_df['significant'].sum()}/{len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "j9zj19dya9m",
   "source": "# Visualize qualifying vs race performance\nfig, ax = plt.subplots(figsize=(14, 10))\n\ncolors_type = perf_df['qualifying_vs_race'].map({\n    'Race Performer': 'green',\n    'Balanced': 'gray',\n    'Qualifying Specialist': 'red'\n})\n\nscatter = ax.scatter(perf_df['avg_grid'], perf_df['avg_finish'],\n                     s=perf_df['races'] * 10, alpha=0.6,\n                     c=colors_type, edgecolors='black', linewidths=1.5)\n\n# Reference line (no change)\nax.plot([0, 20], [0, 20], 'k--', alpha=0.5, linewidth=2, label='No change')\n\n# Label notable drivers\nfor idx, row in perf_df.iterrows():\n    if abs(row['avg_position_change']) > 1.5:  # Label significant performers\n        ax.annotate(row['driver'], (row['avg_grid'], row['avg_finish']),\n                    fontsize=9, ha='center', va='bottom')\n\nax.set_xlabel('Average Grid Position', fontweight='bold')\nax.set_ylabel('Average Finish Position', fontweight='bold')\nax.set_title('Driver Profile: Qualifying vs Race Performance', fontsize=14, fontweight='bold')\nax.grid(alpha=0.3)\n\n# Legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor='green', edgecolor='black', label='Race Performer'),\n    Patch(facecolor='gray', edgecolor='black', label='Balanced'),\n    Patch(facecolor='red', edgecolor='black', label='Qualifying Specialist')\n]\nax.legend(handles=legend_elements, loc='upper left')\n\n# Add quadrant labels\nax.text(2, 18, 'Strong race pace', fontsize=11, style='italic', color='green')\nax.text(18, 2, 'Over-qualify car', fontsize=11, style='italic', color='red')\n\nplt.tight_layout()\nplt.show()\"\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jm865u31t8l",
   "source": "# Calculate qualifying vs race performance for each driver\ndriver_performance = []\n\nfor driver in drivers_with_data:\n    driver_data = df_finished[df_finished['FullName'] == driver].copy()\n    \n    if len(driver_data) < 5:  # Need at least 5 races\n        continue\n    \n    avg_grid = driver_data['GridPosition'].mean()\n    avg_finish = driver_data['Position'].mean()\n    avg_change = driver_data['position_change'].mean()\n    \n    # Qualifying vs race pace ratio\n    # Negative change = losing positions (qualifying specialist)\n    # Positive change = gaining positions (race performer)\n    \n    driver_performance.append({\n        'driver': driver,\n        'races': len(driver_data),\n        'avg_grid': avg_grid,\n        'avg_finish': avg_finish,\n        'avg_position_change': avg_change,\n        'qualifying_vs_race': 'Race Performer' if avg_change > 0.5 else 'Qualifying Specialist' if avg_change < -0.5 else 'Balanced'\n    })\n\nperf_df = pd.DataFrame(driver_performance).sort_values('avg_position_change', ascending=False)\n\nprint(\\\"Driver Performance Profiles:\\\")\\nprint(\\\"=\\\"*70)\\n\\nprint(\\\"\\\\nTop 5 Race Performers (gain positions):\\\")\\ntop_racers = perf_df.head(5)\\nfor idx, row in top_racers.iterrows():\\n    print(f\\\"  {row['driver']}: +{row['avg_position_change']:.2f} positions/race\\\")\\n\\nprint(\\\"\\\\nTop 5 Qualifying Specialists (lose positions):\\\")\\ntop_qualifiers = perf_df.tail(5)\\nfor idx, row in top_qualifiers.iterrows():\\n    print(f\\\"  {row['driver']}: {row['avg_position_change']:.2f} positions/race\\\")\\n\\nprint(f\\\"\\\\n\\\\nPerformance Type Distribution:\\\")\\nprint(perf_df['qualifying_vs_race'].value_counts())\"\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7vm61asw5xj",
   "source": "## Qualifying vs Race Performance\n\nIdentifying qualifying specialists vs race performers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ls3mimnzt0r",
   "source": "# Identify hot streaks - 3+ consecutive improving finishes\ndef identify_streaks(driver_data):\n    streaks = []\n    current_streak = 0\n    \n    for idx in range(1, len(driver_data)):\n        prev_pos = driver_data.iloc[idx-1]['Position']\n        curr_pos = driver_data.iloc[idx]['Position']\n        \n        if curr_pos < prev_pos:  # Improved (lower is better)\n            current_streak += 1\n        else:\n            if current_streak >= 3:\n                streaks.append(current_streak)\n            current_streak = 0\n    \n    if current_streak >= 3:\n        streaks.append(current_streak)\n    \n    return streaks\n\nall_streaks = []\nfor driver in drivers_with_data:\n    driver_data = df_all_sorted[df_all_sorted['FullName'] == driver].copy()\n    streaks = identify_streaks(driver_data)\n    all_streaks.extend(streaks)\n\nprint(f\\\"\\n\\nHot Streak Analysis:\\\")\\nprint(f\\\"Total hot streaks (3+ improving races): {len(all_streaks)}\\\")\\n\\nif len(all_streaks) > 0:\\n    print(f\\\"Average streak length: {np.mean(all_streaks):.1f} races\\\")\\n    print(f\\\"Longest streak: {max(all_streaks)} races\\\")\\n    print(f\\\"\\\\nStreak distribution:\\\")\\n    for length in sorted(set(all_streaks)):\\n        count = all_streaks.count(length)\\n        print(f\\\"  {length} races: {count} occurrences\\\")\\nelse:\\n    print(\\\"No significant hot streaks found\\\")\"\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9oqgumiz8tu",
   "source": "# Test if recent form predicts current performance\nfrom scipy.stats import pearsonr\n\n# Remove first race for each driver (no rolling average yet)\nmomentum_test = momentum_df[momentum_df['rolling_3_finish'].notna()].copy()\n\n# Correlation between rolling average and current finish\ncorr_rolling, p_rolling = pearsonr(\n    momentum_test['rolling_3_finish'], \n    momentum_test['Position']\n)\n\nprint(f\"\\n\\nPredictive Power of Recent Form:\")\nprint(f\\\"Rolling 3-race average vs current finish:\\\")\\nprint(f\\\"  Correlation: {corr_rolling:.4f}\\\")\\nprint(f\\\"  P-value: {p_rolling:.6f}\\\")\\n\\nif p_rolling < 0.05:\\n    print(f\\\"  Result: Recent form significantly predicts current performance\\\")\\nelse:\\n    print(f\\\"  Result: Recent form does not significantly predict performance\\\")\\n\\n# Does momentum (improving/declining) predict next race?\\nmomentum_next = momentum_df.copy()\\nmomentum_next['next_race_finish'] = momentum_next.groupby('FullName')['Position'].shift(-1)\\n\\nmomentum_pred = momentum_next[momentum_next['next_race_finish'].notna()]\\n\\nif len(momentum_pred) > 0:\\n    corr_momentum, p_momentum = pearsonr(\\n        momentum_pred['momentum'].fillna(0),\\n        momentum_pred['next_race_finish']\\n    )\\n    \\n    print(f\\\"\\nMomentum (improvement) vs next race:\\\")\\n    print(f\\\"  Correlation: {corr_momentum:.4f}\\\")\\n    print(f\\\"  P-value: {p_momentum:.6f}\\\")\\n    \\n    if abs(corr_momentum) > 0.1:\\n        print(f\\\"  Result: Momentum shows weak predictive signal\\\")\\n    else:\\n        print(f\\\"  Result: Momentum does not predict next race\\\")\"\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "iad4119siil",
   "source": "# Calculate rolling 3-race performance for each driver\ndf_all_sorted = df_all.sort_values(['FullName', 'year', 'round']).copy()\n\n# Rolling average finish position (last 3 races)\ndf_all_sorted['rolling_3_finish'] = df_all_sorted.groupby('FullName')['Position'].transform(\n    lambda x: x.rolling(window=3, min_periods=1).mean()\n)\n\n# Calculate momentum (improvement or decline)\ndf_all_sorted['prev_race_finish'] = df_all_sorted.groupby('FullName')['Position'].shift(1)\ndf_all_sorted['momentum'] = df_all_sorted['prev_race_finish'] - df_all_sorted['Position']\n\n# Only analyze drivers with at least 10 races\ndriver_race_counts = df_all_sorted.groupby('FullName').size()\ndrivers_with_data = driver_race_counts[driver_race_counts >= 10].index\n\nmomentum_df = df_all_sorted[df_all_sorted['FullName'].isin(drivers_with_data)].copy()\n\nprint(f\"Momentum Analysis Summary:\")\nprint(f\"Drivers analyzed: {len(drivers_with_data)}\\\")\\nprint(f\\\"Total races: {len(momentum_df)}\\\")\\nprint(f\\\"\\\\nOverall momentum statistics:\\\")\\nprint(momentum_df['momentum'].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "phj0usze3dq",
   "source": "## Momentum Analysis\n\nInvestigating team and driver hot streaks and recent form.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ku7ap3daif",
   "source": "# 7. Tukey HSD post-hoc test\nprint(\"\\n7. TUKEY HSD POST-HOC TEST\")\nprint(\"-\" * 70)\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# Test for top 5 grid positions only (to keep output manageable)\ntukey_df = df_finished[df_finished['GridPosition'] <= 5].copy()\n\ntukey_result = pairwise_tukeyhsd(\n    endog=tukey_df['Position'],\n    groups=tukey_df['GridPosition'],\n    alpha=0.05\n)\n\nprint(\"\\nTukey HSD results (P1-P5):\")\nprint(tukey_result)\n\nprint(\"\\n\\nSignificant pairwise differences:\")\ntukey_summary = tukey_result.summary()\nsig_pairs = []\nfor i in range(1, len(tukey_summary.data)):\n    row = tukey_summary.data[i]\n    if row[5] == 'True':  # reject column\n        sig_pairs.append(f\"P{row[0]} vs P{row[1]}\")\n\nif sig_pairs:\n    for pair in sig_pairs[:10]:  # Show first 10\n        print(f\"  {pair}\")\nelse:\n    print(\"  No significant pairwise differences found\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "r1dsyw01a6l",
   "source": "# 6. Two-way ANOVA - circuit AND grid position interaction\nprint(\"\\n6. TWO-WAY ANOVA\")\nprint(\"-\" * 70)\n\nfrom scipy.stats import f_oneway\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n# Create subset for analysis (top 10 grid positions, major circuits)\ntop_circuits = df_finished['circuit'].value_counts().head(10).index\nanalysis_df = df_finished[\n    (df_finished['circuit'].isin(top_circuits)) & \n    (df_finished['GridPosition'] <= 10)\n].copy()\n\n# Fit two-way ANOVA model\nmodel = ols('Position ~ C(GridPosition) + C(circuit) + C(GridPosition):C(circuit)', \n            data=analysis_df).fit()\nanova_results = anova_lm(model, typ=2)\n\nprint(\"\\nTwo-way ANOVA results:\")\nprint(anova_results)\n\nprint(\"\\nInterpretation:\")\nfor factor in ['C(GridPosition)', 'C(circuit)', 'C(GridPosition):C(circuit)']:\n    if factor in anova_results.index:\n        p_val = anova_results.loc[factor, 'PR(>F)']\n        if p_val < 0.05:\n            print(f\"  {factor}: Significant effect (p = {p_val:.6f})\")\n        else:\n            print(f\"  {factor}: No significant effect (p = {p_val:.6f})\")\n\nresults.append({\n    'test': 'Two-way ANOVA',\n    'statistic': anova_results.loc['C(GridPosition):C(circuit)', 'F'] if 'C(GridPosition):C(circuit)' in anova_results.index else 0,\n    'p_value': anova_results.loc['C(GridPosition):C(circuit)', 'PR(>F)'] if 'C(GridPosition):C(circuit)' in anova_results.index else 1,\n    'significant': anova_results.loc['C(GridPosition):C(circuit)', 'PR(>F)'] < 0.05 if 'C(GridPosition):C(circuit)' in anova_results.index else False\n})",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}